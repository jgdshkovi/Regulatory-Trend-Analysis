{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title and Summary extraction via web scraping from sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rss_feeds(feed_links):\n",
    "    feed_data = []\n",
    "    for link in feed_links:\n",
    "        feed = feedparser.parse(link)\n",
    "        for entry in feed.entries:\n",
    "            feed_data.append({\n",
    "                'feed_link': link,\n",
    "                'title': entry.get('title', ''),\n",
    "                'published': entry.get('published', ''),\n",
    "                'summary': entry.get('summary', ''),\n",
    "                'link': entry.get('link', '')\n",
    "            })\n",
    "    return feed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              feed_link  \\\n",
      "0  https://www.justice.gov/news/rss?m=1   \n",
      "1  https://www.justice.gov/news/rss?m=1   \n",
      "2  https://www.justice.gov/news/rss?m=1   \n",
      "3  https://www.justice.gov/news/rss?m=1   \n",
      "4  https://www.justice.gov/news/rss?m=1   \n",
      "\n",
      "                                               title  \\\n",
      "0  Attorney General Merrick B. Garland Statement ...   \n",
      "1  Former CIA Officer Pleads Guilty to Conspiracy...   \n",
      "2  OVW Fiscal Year 2024 Healing and Response Team...   \n",
      "3  California Man Arrested for Making Violent Thr...   \n",
      "4  Readout of Acting Associate Attorney General B...   \n",
      "\n",
      "                         published  \\\n",
      "0  Sat, 25 May 2024 12:00:00 +0000   \n",
      "1  Fri, 24 May 2024 12:00:00 +0000   \n",
      "2  Fri, 24 May 2024 12:00:00 +0000   \n",
      "3  Fri, 24 May 2024 12:00:00 +0000   \n",
      "4  Fri, 24 May 2024 12:00:00 +0000   \n",
      "\n",
      "                                             summary  \\\n",
      "0  The Justice Department issued the following st...   \n",
      "1  Alexander Yuk Ching Ma, 71, of Honolulu, a for...   \n",
      "2                                                      \n",
      "3  A Huntington Beach, California, man was arrest...   \n",
      "4  On Tuesday, May 21, and Wednesday, May 22, Act...   \n",
      "\n",
      "                                                link  \n",
      "0  https://www.justice.gov/opa/pr/attorney-genera...  \n",
      "1  https://www.justice.gov/opa/pr/former-cia-offi...  \n",
      "2  https://www.justice.gov/ovw/video/ovw-fiscal-y...  \n",
      "3  https://www.justice.gov/opa/pr/california-man-...  \n",
      "4  https://www.justice.gov/opa/pr/readout-acting-...  \n"
     ]
    }
   ],
   "source": [
    "rss_feed_links = [\n",
    "    'https://www.justice.gov/news/rss?m=1',\n",
    "    'https://www.sec.gov/news/pressreleases.rss',\n",
    "    'https://www.cftc.gov/RSS/RSSENF/rssenf.xml',\n",
    "    'https://www.ftc.gov/feeds/press-release.xml',\n",
    "    'http://feeds.finra.org/FINRANotices',\n",
    "    'https://www.cftc.gov/RSS/RSSGP/rssgp.xml',\n",
    "    'https://www.justice.gov/news/rss?type=press_release&groupname=441&field_component=1981&search_api_language=en&require_all=1',\n",
    "    'https://www.justice.gov/news/rss?type=press_release&groupname=236&field_component=1751&search_api_language=en&require_all=1',\n",
    "    'https://www.justice.gov/news/rss?type=press_release&groupname=431&field_component=1971&search_api_language=en&require_all=1',\n",
    "    'https://www.justice.gov/news/rss?type=press_release&groupname=291&field_component=1821&search_api_language=en&require_all=1',\n",
    "    'https://www.justice.gov/news/rss?type=press_release&groupname=201&field_component=1721&search_api_language=en&require_all=1',\n",
    "    'https://www.law360.com/whitecollar/rss',\n",
    "    'https://www.whitecollarbriefly.com/feed/',\n",
    "    'https://wp.nyu.edu/compliance_enforcement/feed/',\n",
    "    'https://www.justice.gov/news/rss?end_date=05/08/2024&search_api_fulltext=&sort_by=field_date&start_date=05/01/2024&type=press_release&groupname=291&field_component=1821&search_api_language=en&require_all=0'\n",
    "    # '',\n",
    "    # Add more feed links as needed\n",
    "]\n",
    "\n",
    "# Parse RSS feeds\n",
    "feed_data = parse_rss_feeds(rss_feed_links)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(feed_data)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = 'rss_feeds.xlsx'\n",
    "df.to_excel(excel_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOJ Article scraping and conversion to dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run command pip install openpyxl\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rss_feeds(feed_links, scrape_article=False):\n",
    "    \n",
    "    feed_data = []\n",
    "    \n",
    "    for link in feed_links:\n",
    "        \n",
    "        # Get hostname of provided rss link\n",
    "        urlparser = urlparse(link)\n",
    "        link_hostname = urlparser.hostname\n",
    "        # Get source name of link\n",
    "        link_source = links_dict[link_hostname]\n",
    "\n",
    "        # Parse the rss feed link content\n",
    "        feed = feedparser.parse(link)\n",
    "        \n",
    "        # Iterate feed entries\n",
    "        for entry in feed.entries:\n",
    "            \n",
    "            # Format datetime to yyyy-mm-dd format\n",
    "            date_time = pd.to_datetime(entry.get('published', ''))\n",
    "            date_time = date_time.date()\n",
    "            date_time = pd.Timestamp(date_time.strftime('%Y-%m-%d'))\n",
    "            \n",
    "            # Article source url\n",
    "            url = entry.get('link', '')\n",
    "\n",
    "            # Initialize article content variable\n",
    "            article_content = np.nan\n",
    "\n",
    "            # Check scraping parameter boolean\n",
    "            if scrape_article:\n",
    "                # Check if rss feed source is DOJ (Department of Justice)\n",
    "                if link_source == 'DOJ':\n",
    "                    # Function call to parse article conetent\n",
    "                    article_content = get_rss_article(url, 'DOJ')\n",
    "\n",
    "            # Append feed data objects\n",
    "            feed_data.append({\n",
    "                'feed_link': link,\n",
    "                'feed_source': link_source,\n",
    "                'title': entry.get('title', ''),\n",
    "                'date_published': date_time,\n",
    "                'summary': entry.get('summary', ''),\n",
    "                'link': entry.get('link', ''),\n",
    "                'article_content': article_content\n",
    "            })\n",
    "\n",
    "    # Return feed data objects\n",
    "    return feed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rss_article(url, rss_source):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Check RSS feed source\n",
    "    if rss_source == 'DOJ':\n",
    "        # Scrape the article content via bs4\n",
    "        content_body = soup.find(class_='field_body')\n",
    "        paragraphs = content_body.find_all('p')\n",
    "\n",
    "        # Variable for an observed ending text string\n",
    "        text_strip = 'More information can be found at'\n",
    "        \n",
    "        # Initialize content string\n",
    "        article_content = \"\"\n",
    "        \n",
    "        # Iterate content paragraph tags (i.e <p></p>)\n",
    "        for p in paragraphs:\n",
    "            # Check for any anchor tags (i.e. <a>some link</a>)\n",
    "            for tag in p.findAll('a'):\n",
    "                tag.replaceWithChildren()\n",
    "\n",
    "            # Set variable with paragraph text\n",
    "            p_text = p.text\n",
    "\n",
    "            # Check for ending text string\n",
    "            if text_strip in p_text:\n",
    "                # Get the the index\n",
    "                idx = p_text.index(text_strip)\n",
    "                # Truncate the string on the index\n",
    "                p_text = p_text[:idx]\n",
    "\n",
    "            # Strip whitespace and append to content string \n",
    "            p_text = p_text.strip()\n",
    "            article_content += p_text + \" \"\n",
    "            \n",
    "    # Return article content string\n",
    "    return article_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = {\n",
    "    'www.justice.gov': 'DOJ',\n",
    "    'www.sec.gov': 'SEC',\n",
    "    'www.cftc.gov': 'CFTC',\n",
    "    'www.ftc.gov': 'FTC',\n",
    "    'feeds.finra.org': 'FINRA',\n",
    "    'www.law360.com': 'Law360',\n",
    "    'www.whitecollarbriefly.com': 'White Collar Briefly',\n",
    "    'wp.nyu.edu': 'NYU'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Parse RSS feeds\n",
    "feed_data = parse_rss_feeds(rss_feed_links, scrape_article=True)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(feed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feed_link</th>\n",
       "      <th>feed_source</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>summary</th>\n",
       "      <th>link</th>\n",
       "      <th>article_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.justice.gov/news/rss?m=1</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>Attorney General Merrick B. Garland Statement ...</td>\n",
       "      <td>2024-05-25</td>\n",
       "      <td>The Justice Department issued the following st...</td>\n",
       "      <td>https://www.justice.gov/opa/pr/attorney-genera...</td>\n",
       "      <td>The Justice Department issued the following st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.justice.gov/news/rss?m=1</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>Former CIA Officer Pleads Guilty to Conspiracy...</td>\n",
       "      <td>2024-05-24</td>\n",
       "      <td>Alexander Yuk Ching Ma, 71, of Honolulu, a for...</td>\n",
       "      <td>https://www.justice.gov/opa/pr/former-cia-offi...</td>\n",
       "      <td>Alexander Yuk Ching Ma, 71, of Honolulu, a for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.justice.gov/news/rss?m=1</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>OVW Fiscal Year 2024 Healing and Response Team...</td>\n",
       "      <td>2024-05-24</td>\n",
       "      <td></td>\n",
       "      <td>https://www.justice.gov/ovw/video/ovw-fiscal-y...</td>\n",
       "      <td>OVW conducted a live web-based pre-application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.justice.gov/news/rss?m=1</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>California Man Arrested for Making Violent Thr...</td>\n",
       "      <td>2024-05-24</td>\n",
       "      <td>A Huntington Beach, California, man was arrest...</td>\n",
       "      <td>https://www.justice.gov/opa/pr/california-man-...</td>\n",
       "      <td>A Huntington Beach, California, man was arrest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.justice.gov/news/rss?m=1</td>\n",
       "      <td>DOJ</td>\n",
       "      <td>Readout of Acting Associate Attorney General B...</td>\n",
       "      <td>2024-05-24</td>\n",
       "      <td>On Tuesday, May 21, and Wednesday, May 22, Act...</td>\n",
       "      <td>https://www.justice.gov/opa/pr/readout-acting-...</td>\n",
       "      <td>WASHINGTON – On Tuesday, May 21, and Wednesday...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              feed_link feed_source  \\\n",
       "0  https://www.justice.gov/news/rss?m=1         DOJ   \n",
       "1  https://www.justice.gov/news/rss?m=1         DOJ   \n",
       "2  https://www.justice.gov/news/rss?m=1         DOJ   \n",
       "3  https://www.justice.gov/news/rss?m=1         DOJ   \n",
       "4  https://www.justice.gov/news/rss?m=1         DOJ   \n",
       "\n",
       "                                               title date_published  \\\n",
       "0  Attorney General Merrick B. Garland Statement ...     2024-05-25   \n",
       "1  Former CIA Officer Pleads Guilty to Conspiracy...     2024-05-24   \n",
       "2  OVW Fiscal Year 2024 Healing and Response Team...     2024-05-24   \n",
       "3  California Man Arrested for Making Violent Thr...     2024-05-24   \n",
       "4  Readout of Acting Associate Attorney General B...     2024-05-24   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The Justice Department issued the following st...   \n",
       "1  Alexander Yuk Ching Ma, 71, of Honolulu, a for...   \n",
       "2                                                      \n",
       "3  A Huntington Beach, California, man was arrest...   \n",
       "4  On Tuesday, May 21, and Wednesday, May 22, Act...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.justice.gov/opa/pr/attorney-genera...   \n",
       "1  https://www.justice.gov/opa/pr/former-cia-offi...   \n",
       "2  https://www.justice.gov/ovw/video/ovw-fiscal-y...   \n",
       "3  https://www.justice.gov/opa/pr/california-man-...   \n",
       "4  https://www.justice.gov/opa/pr/readout-acting-...   \n",
       "\n",
       "                                     article_content  \n",
       "0  The Justice Department issued the following st...  \n",
       "1  Alexander Yuk Ching Ma, 71, of Honolulu, a for...  \n",
       "2  OVW conducted a live web-based pre-application...  \n",
       "3  A Huntington Beach, California, man was arrest...  \n",
       "4  WASHINGTON – On Tuesday, May 21, and Wednesday...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display DataFrame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = 'rss_feeds.xlsx'\n",
    "df.to_excel(excel_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Summarization with T5\n",
    "\n",
    "\n",
    "This Jupyter Notebook demonstrates advanced text summarization using the T5 model, which is known for its effectiveness in generating accurate and concise summaries. Key phrase extraction is also performed using the `spaCy` library to enrich our text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Plain typing.NoReturn is not valid as type argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\3178418433.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Imports successful!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_cpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mattributeruler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttributeRuler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdep_parser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0medit_tree_lemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEditTreeLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mentity_linker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mentityruler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEntityRuler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1696\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m     default_error_handler: Callable[\n\u001b[1;32m-> 1698\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"PipeCallable\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Doc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1699\u001b[0m     ],\n\u001b[0;32m   1700\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    753\u001b[0m                                 f\" Got {args}\")\n\u001b[0;32m    754\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem_inner__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0m_tp_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32mpass\u001b[0m  \u001b[1;31m# All real errors (not unhashable args) are raised below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py\u001b[0m in \u001b[0;36m__getitem_inner__\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Callable[args, result]: result must be a type.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_type_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_TypingEllipsis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sreya\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py\u001b[0m in \u001b[0;36m_type_check\u001b[1;34m(arg, msg, is_argument)\u001b[0m\n\u001b[0;32m    133\u001b[0m     if (isinstance(arg, _SpecialForm) and arg is not Any or\n\u001b[0;32m    134\u001b[0m             arg in (Generic, _Protocol)):\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Plain {arg} is not valid as type argument\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Plain typing.NoReturn is not valid as type argument"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "# Load spaCy for key phrase extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the tokenizer with explicit parameters\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large', model_max_length=512, legacy=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12524\\1010847697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load spaCy for key phrase extraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Initialize the tokenizer with explicit parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't5-large'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_max_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_extract_key_phrases(text):\n",
    "    \"\"\"Summarize the text and extract key phrases.\"\"\"\n",
    "    # Generate summary\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def parse_rss_feeds(feed_links):\n",
    "    all_summaries = []\n",
    "    for link in feed_links:\n",
    "        feed = feedparser.parse(link)\n",
    "        for entry in feed.entries:\n",
    "            if 'summary' in entry:\n",
    "                summary = summarize_and_extract_key_phrases(entry['summary'])\n",
    "                all_summaries.append(summary)\n",
    "    return all_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  a federal jury in Charlotte, north carolina, c...\n",
      "1  a national of the Dominican republic was sente...\n",
      "2  seven members and associates of the sinaloa ca...\n",
      "3  a former Mississippi bureau of investigations ...\n",
      "4  seven defendants were sentenced yesterday and ...\n"
     ]
    }
   ],
   "source": [
    "# Example feed links to parse\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "rss_feed_links = [\n",
    "    'https://www.justice.gov/news/rss?m=1',\n",
    "    'https://www.sec.gov/news/pressreleases.rss'\n",
    "    # Add more links as needed\n",
    "]\n",
    "\n",
    "# Parse RSS feeds and print results\n",
    "feed_data = parse_rss_feeds(rss_feed_links)\n",
    "df = pd.DataFrame(feed_data)\n",
    "print(df.head())  # Display first few rows of the DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis of Summarized Texts\n",
    "\n",
    "This notebook applies TF-IDF analysis to the summaries generated from a set of texts. The frequency of keywords in each summary is produced helping to highlight the most significant words in each summarized document.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sreya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# You may need to download NLTK's stopword list\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assuming the summarize_text and extract_key_phrases functions are defined as per previous discussions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1:\n",
      "bils  -  0.31\n",
      "russell  -  0.31\n",
      "diego  -  0.31\n",
      "san  -  0.31\n",
      "violence  -  0.16\n",
      "crime  -  0.16\n",
      "relation  -  0.16\n",
      "firearm  -  0.16\n",
      "discharging  -  0.16\n",
      "officers  -  0.16\n",
      "\n",
      "\n",
      "Summary 2:\n",
      "scams  -  0.25\n",
      "proceeds  -  0.25\n",
      "launder  -  0.25\n",
      "roles  -  0.25\n",
      "leading  -  0.25\n",
      "played  -  0.25\n",
      "alleging  -  0.25\n",
      "nationals  -  0.25\n",
      "chinese  -  0.25\n",
      "central  -  0.25\n",
      "\n",
      "\n",
      "Summary 3:\n",
      "mortgage  -  0.28\n",
      "clients  -  0.28\n",
      "documents  -  0.28\n",
      "creating  -  0.28\n",
      "defraud  -  0.28\n",
      "builder  -  0.28\n",
      "obtain  -  0.25\n",
      "conspiring  -  0.25\n",
      "home  -  0.25\n",
      "help  -  0.23\n",
      "\n",
      "\n",
      "Summary 4:\n",
      "man  -  0.52\n",
      "murder  -  0.26\n",
      "related  -  0.26\n",
      "hawaii  -  0.26\n",
      "appearance  -  0.26\n",
      "initial  -  0.26\n",
      "trafficking  -  0.24\n",
      "connection  -  0.24\n",
      "face  -  0.24\n",
      "drug  -  0.22\n",
      "\n",
      "\n",
      "Summary 5:\n",
      "korea  -  0.45\n",
      "seizures  -  0.25\n",
      "dprk  -  0.23\n",
      "democratic  -  0.23\n",
      "generation  -  0.23\n",
      "revenue  -  0.23\n",
      "illicit  -  0.23\n",
      "disrupt  -  0.23\n",
      "actions  -  0.23\n",
      "authorized  -  0.23\n",
      "\n",
      "\n",
      "Summary 6:\n",
      "procedures  -  0.32\n",
      "medicare  -  0.32\n",
      "claims  -  0.29\n",
      "suitability  -  0.16\n",
      "patient  -  0.16\n",
      "evaluate  -  0.16\n",
      "required  -  0.16\n",
      "hospitals  -  0.16\n",
      "way  -  0.16\n",
      "specifying  -  0.16\n",
      "\n",
      "\n",
      "Summary 7:\n",
      "immigration  -  0.3\n",
      "swiftly  -  0.22\n",
      "docket  -  0.22\n",
      "security  -  0.22\n",
      "homeland  -  0.22\n",
      "justice  -  0.19\n",
      "department  -  0.18\n",
      "proceedings  -  0.11\n",
      "case  -  0.11\n",
      "efficient  -  0.11\n",
      "\n",
      "\n",
      "Summary 8:\n",
      "hiv  -  0.32\n",
      "da  -  0.32\n",
      "tennessee  -  0.32\n",
      "law  -  0.29\n",
      "living  -  0.29\n",
      "harsher  -  0.16\n",
      "discriminatory  -  0.16\n",
      "subjected  -  0.16\n",
      "discrimination  -  0.16\n",
      "correct  -  0.16\n",
      "\n",
      "\n",
      "Summary 9:\n",
      "schedule  -  0.4\n",
      "rulemaking  -  0.4\n",
      "csa  -  0.2\n",
      "substances  -  0.2\n",
      "controlled  -  0.2\n",
      "iii  -  0.2\n",
      "marijuana  -  0.2\n",
      "moving  -  0.2\n",
      "consider  -  0.2\n",
      "formal  -  0.2\n",
      "\n",
      "\n",
      "Summary 10:\n",
      "workers  -  0.26\n",
      "coordinated  -  0.26\n",
      "technology  -  0.23\n",
      "series  -  0.23\n",
      "dprk  -  0.23\n",
      "korea  -  0.23\n",
      "democratic  -  0.23\n",
      "generation  -  0.23\n",
      "revenue  -  0.23\n",
      "illicit  -  0.23\n",
      "\n",
      "\n",
      "Summary 11:\n",
      "company  -  0.33\n",
      "contributions  -  0.21\n",
      "campaign  -  0.21\n",
      "improper  -  0.21\n",
      "accounts  -  0.21\n",
      "expenditure  -  0.21\n",
      "bribery  -  0.21\n",
      "orchestrating  -  0.21\n",
      "retrial  -  0.21\n",
      "consultant  -  0.21\n",
      "\n",
      "\n",
      "Summary 12:\n",
      "importation  -  0.24\n",
      "unlawful  -  0.24\n",
      "kilograms  -  0.24\n",
      "385  -  0.24\n",
      "distribute  -  0.24\n",
      "months  -  0.24\n",
      "years  -  0.24\n",
      "dominican  -  0.24\n",
      "cocaine  -  0.22\n",
      "international  -  0.22\n",
      "\n",
      "\n",
      "Summary 13:\n",
      "matter  -  0.24\n",
      "previously  -  0.24\n",
      "methamphetamine  -  0.24\n",
      "fentanyl  -  0.24\n",
      "cartel  -  0.24\n",
      "sinaloa  -  0.24\n",
      "associates  -  0.24\n",
      "members  -  0.24\n",
      "seven  -  0.22\n",
      "cocaine  -  0.22\n",
      "\n",
      "\n",
      "Summary 14:\n",
      "force  -  0.39\n",
      "arrestee  -  0.23\n",
      "handcuffed  -  0.23\n",
      "task  -  0.23\n",
      "fugitive  -  0.23\n",
      "coast  -  0.23\n",
      "gulf  -  0.23\n",
      "marshals  -  0.23\n",
      "assigned  -  0.23\n",
      "officer  -  0.23\n",
      "\n",
      "\n",
      "Summary 15:\n",
      "clinic  -  0.35\n",
      "following  -  0.35\n",
      "oct  -  0.17\n",
      "health  -  0.17\n",
      "reproductive  -  0.17\n",
      "area  -  0.17\n",
      "washington  -  0.17\n",
      "blockade  -  0.17\n",
      "obstruction  -  0.17\n",
      "physical  -  0.17\n",
      "\n",
      "\n",
      "Summary 16:\n",
      "mexico  -  0.36\n",
      "office  -  0.27\n",
      "fgr  -  0.26\n",
      "assistance  -  0.26\n",
      "city  -  0.24\n",
      "international  -  0.24\n",
      "department  -  0.15\n",
      "prosecutors  -  0.13\n",
      "workshops  -  0.13\n",
      "aires  -  0.13\n",
      "\n",
      "\n",
      "Summary 17:\n",
      "maxim  -  0.46\n",
      "permission  -  0.23\n",
      "healthcare  -  0.23\n",
      "agreement  -  0.23\n",
      "work  -  0.21\n",
      "company  -  0.18\n",
      "justice  -  0.13\n",
      "department  -  0.13\n",
      "unnecessary  -  0.11\n",
      "continued  -  0.11\n",
      "\n",
      "\n",
      "Summary 18:\n",
      "bueno  -  0.38\n",
      "peraire  -  0.28\n",
      "york  -  0.26\n",
      "new  -  0.19\n",
      "judge  -  0.19\n",
      "magistrate  -  0.19\n",
      "approximately  -  0.19\n",
      "fraud  -  0.19\n",
      "wire  -  0.19\n",
      "commit  -  0.19\n",
      "\n",
      "\n",
      "Summary 19:\n",
      "youth  -  0.48\n",
      "justice  -  0.27\n",
      "department  -  0.27\n",
      "juvenile  -  0.24\n",
      "kentucky  -  0.24\n",
      "run  -  0.24\n",
      "center  -  0.24\n",
      "centers  -  0.24\n",
      "detention  -  0.24\n",
      "conditions  -  0.24\n",
      "\n",
      "\n",
      "Summary 20:\n",
      "\n",
      "\n",
      "Summary 21:\n",
      "maryland  -  0.45\n",
      "electrical  -  0.27\n",
      "destroy  -  0.27\n",
      "damage  -  0.27\n",
      "catonsville  -  0.27\n",
      "36  -  0.27\n",
      "clendaniel  -  0.27\n",
      "beth  -  0.27\n",
      "sarah  -  0.27\n",
      "facilities  -  0.24\n",
      "\n",
      "\n",
      "Summary 22:\n",
      "leadership  -  0.38\n",
      "introduction  -  0.38\n",
      "generous  -  0.38\n",
      "shaheena  -  0.38\n",
      "thank  -  0.38\n",
      "good  -  0.38\n",
      "afternoon  -  0.35\n",
      "\n",
      "\n",
      "Summary 23:\n",
      "community  -  0.29\n",
      "em  -  0.29\n",
      "smi  -  0.29\n",
      "nebraska  -  0.29\n",
      "facilities  -  0.27\n",
      "people  -  0.23\n",
      "department  -  0.16\n",
      "live  -  0.15\n",
      "need  -  0.15\n",
      "critical  -  0.15\n",
      "\n",
      "\n",
      "Summary 24:\n",
      "apd  -  0.38\n",
      "decree  -  0.26\n",
      "consent  -  0.26\n",
      "motion  -  0.26\n",
      "albuquerque  -  0.26\n",
      "compliance  -  0.23\n",
      "joint  -  0.23\n",
      "filed  -  0.23\n",
      "city  -  0.23\n",
      "department  -  0.14\n",
      "\n",
      "\n",
      "Summary 25:\n",
      "laws  -  0.29\n",
      "voting  -  0.29\n",
      "ensure  -  0.29\n",
      "george  -  0.29\n",
      "prince  -  0.29\n",
      "elections  -  0.29\n",
      "compliance  -  0.26\n",
      "monitor  -  0.26\n",
      "14  -  0.26\n",
      "rights  -  0.26\n",
      "\n",
      "\n",
      "Summary 26:\n",
      "heather  -  0.4\n",
      "policy  -  0.4\n",
      "director  -  0.33\n",
      "gensler  -  0.2\n",
      "gary  -  0.2\n",
      "chair  -  0.2\n",
      "counsel  -  0.2\n",
      "finance  -  0.2\n",
      "corporation  -  0.2\n",
      "recently  -  0.2\n",
      "\n",
      "\n",
      "Summary 27:\n",
      "diamantopoulos  -  0.44\n",
      "regional  -  0.4\n",
      "office  -  0.29\n",
      "veteran  -  0.22\n",
      "year  -  0.22\n",
      "ms  -  0.22\n",
      "largest  -  0.22\n",
      "chicago  -  0.22\n",
      "named  -  0.22\n",
      "tina  -  0.22\n",
      "\n",
      "\n",
      "Summary 28:\n",
      "institutions  -  0.27\n",
      "personal  -  0.27\n",
      "consumers  -  0.27\n",
      "treatment  -  0.27\n",
      "govern  -  0.27\n",
      "enhance  -  0.27\n",
      "modernize  -  0.27\n",
      "adoption  -  0.27\n",
      "nonpublic  -  0.24\n",
      "regulation  -  0.24\n",
      "\n",
      "\n",
      "Summary 29:\n",
      "report  -  0.38\n",
      "new  -  0.33\n",
      "investment  -  0.29\n",
      "adv  -  0.24\n",
      "updated  -  0.22\n",
      "form  -  0.22\n",
      "data  -  0.22\n",
      "aggregated  -  0.22\n",
      "statistics  -  0.22\n",
      "staff  -  0.22\n",
      "\n",
      "\n",
      "Summary 30:\n",
      "recommendations  -  0.26\n",
      "advice  -  0.26\n",
      "provides  -  0.26\n",
      "vacancies  -  0.26\n",
      "number  -  0.26\n",
      "limited  -  0.26\n",
      "regulations  -  0.23\n",
      "candidates  -  0.23\n",
      "agency  -  0.23\n",
      "committee  -  0.22\n",
      "\n",
      "\n",
      "Summary 31:\n",
      "failing  -  0.26\n",
      "duties  -  0.26\n",
      "fiduciary  -  0.26\n",
      "breaching  -  0.26\n",
      "conover  -  0.26\n",
      "christopher  -  0.26\n",
      "wealth  -  0.26\n",
      "hudson  -  0.26\n",
      "valley  -  0.23\n",
      "york  -  0.23\n",
      "\n",
      "\n",
      "Summary 32:\n",
      "advisers  -  0.36\n",
      "reporting  -  0.25\n",
      "exempt  -  0.25\n",
      "rias  -  0.25\n",
      "jointly  -  0.25\n",
      "fincen  -  0.25\n",
      "network  -  0.25\n",
      "crimes  -  0.25\n",
      "treasury  -  0.25\n",
      "require  -  0.22\n",
      "\n",
      "\n",
      "Summary 33:\n",
      "trading  -  0.39\n",
      "information  -  0.36\n",
      "realizing  -  0.21\n",
      "goods  -  0.21\n",
      "sporting  -  0.21\n",
      "dick  -  0.21\n",
      "concerning  -  0.21\n",
      "material  -  0.21\n",
      "misappropriating  -  0.21\n",
      "jr  -  0.21\n",
      "\n",
      "\n",
      "Summary 34:\n",
      "omwi  -  0.44\n",
      "benjamin  -  0.44\n",
      "americorps  -  0.24\n",
      "mr  -  0.24\n",
      "nathaniel  -  0.24\n",
      "diversity  -  0.22\n",
      "chief  -  0.22\n",
      "join  -  0.22\n",
      "inclusion  -  0.22\n",
      "women  -  0.22\n",
      "\n",
      "\n",
      "Summary 35:\n",
      "borgers  -  0.43\n",
      "pcaob  -  0.21\n",
      "board  -  0.21\n",
      "oversight  -  0.21\n",
      "accounting  -  0.21\n",
      "systemic  -  0.21\n",
      "deliberate  -  0.21\n",
      "respondents  -  0.21\n",
      "owner  -  0.21\n",
      "pc  -  0.21\n",
      "\n",
      "\n",
      "Summary 36:\n",
      "meeting  -  0.46\n",
      "activity  -  0.23\n",
      "cf  -  0.23\n",
      "reg  -  0.23\n",
      "crowdfunding  -  0.23\n",
      "discussion  -  0.23\n",
      "include  -  0.23\n",
      "monday  -  0.23\n",
      "agenda  -  0.23\n",
      "released  -  0.23\n",
      "\n",
      "\n",
      "Summary 37:\n",
      "report  -  0.36\n",
      "registered  -  0.34\n",
      "new  -  0.31\n",
      "quarterly  -  0.23\n",
      "port  -  0.23\n",
      "funds  -  0.23\n",
      "reported  -  0.23\n",
      "fund  -  0.23\n",
      "updated  -  0.21\n",
      "form  -  0.21\n",
      "\n",
      "\n",
      "Summary 38:\n",
      "conference  -  0.46\n",
      "management  -  0.36\n",
      "variety  -  0.23\n",
      "bring  -  0.23\n",
      "thursday  -  0.23\n",
      "asset  -  0.23\n",
      "trends  -  0.23\n",
      "emerging  -  0.23\n",
      "im  -  0.23\n",
      "division  -  0.23\n",
      "\n",
      "\n",
      "Summary 39:\n",
      "continue  -  0.24\n",
      "organization  -  0.24\n",
      "regulatory  -  0.24\n",
      "self  -  0.24\n",
      "consequently  -  0.24\n",
      "dollars  -  0.24\n",
      "80  -  0.24\n",
      "27  -  0.24\n",
      "set  -  0.24\n",
      "transactions  -  0.24\n",
      "\n",
      "\n",
      "Summary 40:\n",
      "charges  -  0.33\n",
      "combined  -  0.26\n",
      "000  -  0.26\n",
      "200  -  0.26\n",
      "firms  -  0.26\n",
      "violations  -  0.26\n",
      "marketing  -  0.26\n",
      "penalties  -  0.24\n",
      "settle  -  0.22\n",
      "settled  -  0.22\n",
      "\n",
      "\n",
      "Summary 41:\n",
      "business  -  0.49\n",
      "formation  -  0.32\n",
      "small  -  0.32\n",
      "capital  -  0.3\n",
      "18  -  0.19\n",
      "sessions  -  0.19\n",
      "virtual  -  0.19\n",
      "forum  -  0.19\n",
      "government  -  0.19\n",
      "43rd  -  0.19\n",
      "\n",
      "\n",
      "Summary 42:\n",
      "senvest  -  0.49\n",
      "communications  -  0.25\n",
      "electronic  -  0.25\n",
      "preserve  -  0.25\n",
      "maintain  -  0.25\n",
      "longstanding  -  0.25\n",
      "widespread  -  0.25\n",
      "llc  -  0.22\n",
      "failures  -  0.22\n",
      "adviser  -  0.19\n",
      "\n",
      "\n",
      "Summary 43:\n",
      "financial  -  0.37\n",
      "answer  -  0.22\n",
      "plan  -  0.22\n",
      "having  -  0.22\n",
      "like  -  0.22\n",
      "look  -  0.22\n",
      "future  -  0.22\n",
      "does  -  0.22\n",
      "month  -  0.22\n",
      "capability  -  0.22\n",
      "\n",
      "\n",
      "Summary 44:\n",
      "internet  -  0.45\n",
      "amendments  -  0.41\n",
      "adviser  -  0.36\n",
      "investment  -  0.27\n",
      "relying  -  0.23\n",
      "exemption  -  0.23\n",
      "permitting  -  0.23\n",
      "adopted  -  0.23\n",
      "require  -  0.21\n",
      "register  -  0.19\n",
      "\n",
      "\n",
      "Summary 45:\n",
      "committee  -  0.39\n",
      "reform  -  0.24\n",
      "street  -  0.24\n",
      "wall  -  0.24\n",
      "dodd  -  0.24\n",
      "established  -  0.24\n",
      "improve  -  0.24\n",
      "investors  -  0.24\n",
      "protect  -  0.24\n",
      "appointment  -  0.24\n",
      "\n",
      "\n",
      "Summary 46:\n",
      "bechtolsheim  -  0.46\n",
      "charges  -  0.28\n",
      "networks  -  0.23\n",
      "arista  -  0.23\n",
      "silicon  -  0.23\n",
      "architect  -  0.23\n",
      "andy  -  0.23\n",
      "andreas  -  0.23\n",
      "chief  -  0.21\n",
      "trading  -  0.21\n",
      "\n",
      "\n",
      "Summary 47:\n",
      "diversity  -  0.41\n",
      "inclusion  -  0.41\n",
      "assessment  -  0.23\n",
      "2022  -  0.23\n",
      "reports  -  0.23\n",
      "entities  -  0.23\n",
      "regulated  -  0.23\n",
      "equity  -  0.23\n",
      "continuing  -  0.23\n",
      "omwi  -  0.2\n",
      "\n",
      "\n",
      "Summary 48:\n",
      "georgia  -  0.42\n",
      "state  -  0.38\n",
      "north  -  0.35\n",
      "college  -  0.21\n",
      "dalton  -  0.21\n",
      "university  -  0.21\n",
      "nasaa  -  0.21\n",
      "association  -  0.21\n",
      "administrators  -  0.21\n",
      "american  -  0.21\n",
      "\n",
      "\n",
      "Summary 49:\n",
      "unregistered  -  0.24\n",
      "engaged  -  0.24\n",
      "injunction  -  0.24\n",
      "imposing  -  0.24\n",
      "penalty  -  0.24\n",
      "civil  -  0.24\n",
      "21  -  0.24\n",
      "ordering  -  0.24\n",
      "judgment  -  0.24\n",
      "genesis  -  0.24\n",
      "\n",
      "\n",
      "Summary 50:\n",
      "ai  -  0.26\n",
      "intelligence  -  0.26\n",
      "artificial  -  0.26\n",
      "purported  -  0.26\n",
      "statements  -  0.26\n",
      "misleading  -  0.26\n",
      "making  -  0.26\n",
      "predictions  -  0.26\n",
      "usa  -  0.26\n",
      "delphia  -  0.26\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import feedparser\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# RSS feed links\n",
    "rss_feed_links = [\n",
    "    'https://www.justice.gov/news/rss?m=1',\n",
    "    'https://www.sec.gov/news/pressreleases.rss'\n",
    "]\n",
    "\n",
    "# Function to parse RSS feeds and extract summaries\n",
    "def parse_rss_feeds(links):\n",
    "    summaries = []\n",
    "    for link in links:\n",
    "        feed = feedparser.parse(link)\n",
    "        for entry in feed.entries:\n",
    "            if 'summary' in entry:\n",
    "                summaries.append(entry.summary)\n",
    "            elif 'description' in entry:\n",
    "                summaries.append(entry.description)\n",
    "    return summaries\n",
    "\n",
    "# Parse RSS feeds and obtain summaries\n",
    "all_summaries = parse_rss_feeds(rss_feed_links)\n",
    "\n",
    "# Transform summaries to TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(all_summaries)\n",
    "\n",
    "# Function to display top keywords for each summary\n",
    "def display_top_keywords(tfidf_matrix, feature_names, top_n=10):\n",
    "    for idx, summary in enumerate(all_summaries):\n",
    "        print(f\"Summary {idx+1}:\")\n",
    "        feature_index = tfidf_matrix[idx, :].nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[idx, x] for x in feature_index])\n",
    "        sorted_items = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "        for item in sorted_items[:top_n]:\n",
    "            print(feature_names[item[0]], ' - ', round(item[1], 2))\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Get feature names and display keywords\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "display_top_keywords(tfidf_matrix, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with RoBERTa\n",
    "\n",
    "This notebook demonstrates how to use the RoBERTa model for classifying text into predefined categories such as animals, travel, technology, fashion, culture, weather, and food. We aim for high accuracy in classification by leveraging the powerful capabilities of the RoBERTa model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Libraries are properly installed.\n"
     ]
    }
   ],
   "source": [
    "# Try importing the libraries\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    from huggingface_hub import HfApi  # Not directly used here, but good to test\n",
    "    print(\"Success: Libraries are properly installed.\")\n",
    "except ImportError as e:\n",
    "    print(\"Failed to import libraries:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   labels\n",
      "0        Accounting fraud\n",
      "1    Antitrust violations\n",
      "2  Asset misappropriation\n",
      "3         Asset stripping\n",
      "4         Bait and switch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'C:\\\\Users\\\\sreya\\\\OneDrive\\\\Desktop\\\\FADS\\\\Regulatortrendscode\\\\Corpoarate Compliance Terms.csv'\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Rename the column\n",
    "data.columns = ['labels']\n",
    "\n",
    "# Display the renamed DataFrame\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5eda818dd74a0fb2ac16871d1b5d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['summary', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 67\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "# Function to tokenize the summaries\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['summary'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization to your dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Display the tokenized datasets\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.35.2\n",
      "Torch version: 2.1.0+cpu\n",
      "Accelerate version: 0.30.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Accelerate version:\", accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "\n",
    "# RSS feed links\n",
    "rss_feed_links = [\n",
    "    'https://www.justice.gov/news/rss?m=1',\n",
    "    'https://www.sec.gov/news/pressreleases.rss'\n",
    "]\n",
    "\n",
    "# Function to parse RSS feeds and extract summaries\n",
    "def parse_rss_feeds(links):\n",
    "    summaries = []\n",
    "    links_list = []\n",
    "    for link in links:\n",
    "        feed = feedparser.parse(link)\n",
    "        for entry in feed.entries:\n",
    "            if 'summary' in entry:\n",
    "                summaries.append(entry.summary)\n",
    "                links_list.append(entry.link)\n",
    "            elif 'description' in entry:\n",
    "                summaries.append(entry.description)\n",
    "                links_list.append(entry.link)\n",
    "    return summaries, links_list\n",
    "\n",
    "# Parse RSS feeds and obtain summaries\n",
    "all_summaries, all_links = parse_rss_feeds(rss_feed_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Load the compliance terms CSV file\n",
    "compliance_file_path = 'C:\\\\Users\\\\sreya\\\\OneDrive\\\\Desktop\\\\FADS\\\\Regulatortrendscode\\\\Corpoarate Compliance Terms.csv'\n",
    "compliance_terms = pd.read_csv(compliance_file_path, header=None)[0].tolist()\n",
    "\n",
    "# Function to check if a summary is compliant\n",
    "def is_compliant(summary, terms):\n",
    "    for term in terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', summary, re.IGNORECASE):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Generate labels for all summaries\n",
    "labels = [is_compliant(summary, compliance_terms) for summary in all_summaries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load the modified CSV file without column names\n",
    "file_path = 'C:\\\\Users\\\\sreya\\\\OneDrive\\\\Desktop\\\\FADS\\\\Regulatortrendscode\\\\summaries_with_labels.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the 'summary' column is of type string and 'label' column is of type int\n",
    "data['summary'] = data['summary'].astype(str)\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Split data into training and evaluation sets\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "eval_data = data.drop(train_data.index)\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(eval_data)\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"eval\": eval_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85afc6ed32a249289e6f9015e91174fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e08f962e6a4881852abd2d677bce72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d231a4d530a4106975df33936e453e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc6cad55b8143b5a00f05a7c1a60a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7868456840515137, 'eval_runtime': 32.2853, 'eval_samples_per_second': 0.31, 'eval_steps_per_second': 0.062, 'epoch': 1.0}\n",
      "{'loss': 0.7738, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d59a6300514aec811052903e4ee165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7214387655258179, 'eval_runtime': 36.2457, 'eval_samples_per_second': 0.276, 'eval_steps_per_second': 0.055, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193fd9609df94a6ba0d7347add8864fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6529731750488281, 'eval_runtime': 29.5856, 'eval_samples_per_second': 0.338, 'eval_steps_per_second': 0.068, 'epoch': 3.0}\n",
      "{'train_runtime': 2113.6851, 'train_samples_per_second': 0.057, 'train_steps_per_second': 0.007, 'train_loss': 0.7565571626027425, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1c6b6820cc4264913bc14c20beccea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6529731750488281,\n",
       " 'eval_runtime': 29.8236,\n",
       " 'eval_samples_per_second': 0.335,\n",
       " 'eval_steps_per_second': 0.067,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "# Function to tokenize the summaries\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['summary'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization to your dataset\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load a pre-trained RoBERTa model for sequence classification with 2 labels\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size per device during evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     # evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['eval'],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough summaries for all labels. Please provide more summaries or fewer labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809c6d1b230e49c7b3fe81916afe9187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Apply the tokenization to your dataset\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Load a pre-trained RoBERTa model for sequence classification with 2 labels\u001b[39;00m\n\u001b[0;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m RobertaForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-large\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\dataset_dict.py:869\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 869\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    891\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\dataset_dict.py:870\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    869\u001b[0m     {\n\u001b[1;32m--> 870\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    890\u001b[0m     }\n\u001b[0;32m    891\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3158\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3543\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3544\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3545\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3547\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3556\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3420\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m, in \u001b[0;36mtokenize_function\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2537\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2538\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2624\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2619\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2620\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2621\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2622\u001b[0m         )\n\u001b[0;32m   2623\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2645\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2646\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2662\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2663\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2815\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2806\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2807\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2808\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2812\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2813\u001b[0m )\n\u001b[1;32m-> 2815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:733\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 733\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    735\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\sreya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:713\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    715\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load the compliance terms CSV file without column names\n",
    "file_path = 'C:\\\\Users\\\\sreya\\\\OneDrive\\\\Desktop\\\\FADS\\\\Regulatortrendscode\\\\Corpoarate Compliance Terms.csv'\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# Rename the single column to 'label'\n",
    "data.columns = ['label']\n",
    "\n",
    "# Load the summaries CSV file\n",
    "summaries_file_path = 'C:\\\\Users\\\\sreya\\\\OneDrive\\\\Desktop\\\\FADS\\\\Regulatortrendscode\\\\summaries.csv'\n",
    "summaries_df = pd.read_csv(summaries_file_path)\n",
    "\n",
    "# Ensure the number of summaries matches the number of labels in your CSV\n",
    "if len(summaries_df) < len(data):\n",
    "    print(\"Not enough summaries for all labels. Please provide more summaries or fewer labels.\")\n",
    "    exit()\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "data['summary'] = summaries_df['summary'][:len(data)]\n",
    "\n",
    "# Reorder columns to have 'summary' first and 'label' second\n",
    "data = data[['summary', 'label']]\n",
    "\n",
    "# Save the modified CSV file\n",
    "modified_file_path = 'C:\\\\Users\\\\sreya\\\\OneDrive\\\\Desktop\\\\FADS\\\\Regulatortrendscode\\\\modified_compliance_terms.csv'\n",
    "data.to_csv(modified_file_path, index=False, header=False)\n",
    "\n",
    "# Load the modified CSV file without column names\n",
    "data = pd.read_csv(modified_file_path, header=None)\n",
    "\n",
    "# Rename the columns\n",
    "data.columns = ['summary', 'label']\n",
    "\n",
    "# Split data into training and evaluation sets\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "eval_data = data.drop(train_data.index)\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(eval_data)\n",
    "datasets = DatasetDict({\"train\": train_dataset, \"eval\": eval_dataset})\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "# Function to tokenize the summaries\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['summary'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization to your dataset\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load a pre-trained RoBERTa model for sequence classification with 2 labels\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size per device during evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     # evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['eval'],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('./my_roberta_model')\n",
    "\n",
    "# To evaluate the model, you can add an evaluation dataset in the Trainer and use `trainer.evaluate()`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
