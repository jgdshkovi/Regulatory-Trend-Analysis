{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1117, which is longer than the specified 1000\n",
      "Created a chunk of size 1117, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justice Department Secures Agreement with Climate Nonprofit to Resolve Claims of Employment Discrimination. The Justice Department announced today that it secured a settlement agreement with Second Nature, a non-profit organization based in Massachusetts. The agreement resolves the departmentâ€™s determination that Second Nature violated the Immigration and Nationality Act (INA) by posting discriminatory job advertisements that deterred non-U.S. citizens from applying for open positions.\n"
     ]
    }
   ],
   "source": [
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"ccc.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "\n",
    "# # Sample Data (Replace with your actual data)\n",
    "# data = [\n",
    "#     {\"id\": 1, \"summary\": \"Company X fined for environmental violations\", \"date\": datetime(2023, 1, 1)},\n",
    "#     {\"id\": 2, \"summary\": \"New regulations proposed for anti-bribery compliance by government Y\", \"date\": datetime(2023, 2, 15)},\n",
    "#     {\"id\": 3, \"summary\": \"Allegations of fraud surface against CEO John Doe of company Z\", \"date\": datetime(2023, 3, 10)},\n",
    "#     # ... more data entries\n",
    "# ]\n",
    "\n",
    "# # Convert data to Pandas DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = 'summary1.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "\n",
    "# Define time period (replace with your desired start and end dates)\n",
    "# start_date = datetime(2023, 1, 1)\n",
    "# end_date = datetime(2023, 6, 30)\n",
    "\n",
    "# # Filter data for the time period\n",
    "# df_filtered = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "df_filtered['summary'] = df['summary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           SEC Charges Lordstown Motors with Misleading I...\n",
       "1           SEC Investor Advisory Committee to Discuss Pro...\n",
       "2           Federal Court Orders Unregistered Pool Operato...\n",
       "3           Federal Court Orders California-Based Precious...\n",
       "4           CFTC Approves Final Rules on Swap Confirmation...\n",
       "                                  ...                        \n",
       "231         Readout of Justice Department’s Civil Rights D...\n",
       "232         Elara Caring Agrees to Pay $4.2 Million to Set...\n",
       "233         Scheme to Transfer Money to Iran Results in Gu...\n",
       "summary     0      SEC Charges Lordstown Motors with Misle...\n",
       "summary1    0      SEC Charges Lordstown Motors with Misle...\n",
       "Name: summary, Length: 236, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E178] Each pattern should be a list of dicts, but got: {'POS': 'PROPN'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('ORG', [pattern])`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 25\u001b[0m\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_text, [ent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments]  \u001b[38;5;66;03m# Return cleaned text and extracted entities\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to summaries and extract entities\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# df[\"summary\"], df[\"entities\"] = zip(*df[\"summary\"].apply(preprocess_text))\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply preprocessing with a temporary DataFrame\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# temp_df = df[[\"summary\"]].apply(preprocess_text, axis=1, result_type='expand')\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Assign results back to original DataFrame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m temp_df[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jgdsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jgdsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jgdsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\jgdsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[37], line 25\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_text, [ent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments]  \u001b[38;5;66;03m# Return cleaned text and extracted entities\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to summaries and extract entities\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# df[\"summary\"], df[\"entities\"] = zip(*df[\"summary\"].apply(preprocess_text))\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply preprocessing with a temporary DataFrame\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# temp_df = df[[\"summary\"]].apply(preprocess_text, axis=1, result_type='expand')\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Assign results back to original DataFrame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m temp_df[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[37], line 12\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m org_pattern \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROPN\u001b[39m\u001b[38;5;124m'\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEMMA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minc\u001b[39m\u001b[38;5;124m'\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEMMA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mltd\u001b[39m\u001b[38;5;124m'\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEMMA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllc\u001b[39m\u001b[38;5;124m'\u001b[39m}]\n\u001b[0;32m     11\u001b[0m person_pattern \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROPN\u001b[39m\u001b[38;5;124m'\u001b[39m}]\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mORG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morg_pattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wrap org_pattern in a list\u001b[39;00m\n\u001b[0;32m     13\u001b[0m matcher\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERSON\u001b[39m\u001b[38;5;124m\"\u001b[39m, person_pattern)  \u001b[38;5;66;03m# Wrap person_pattern in a list\u001b[39;00m\n\u001b[0;32m     15\u001b[0m doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;241m=\u001b[39m [ent \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;28;01mif\u001b[39;00m ent\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERSON\u001b[39m\u001b[38;5;124m\"\u001b[39m)]  \u001b[38;5;66;03m# Extract relevant entities\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jgdsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:125\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E178] Each pattern should be a list of dicts, but got: {'POS': 'PROPN'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('ORG', [pattern])`"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"\n",
    "  Preprocess text with spaCy for NER (replace with your desired cleaning steps)\n",
    "  \"\"\"\n",
    "  nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy model\n",
    "  doc = nlp(text)\n",
    "\n",
    "  # Define matcher for named entities (organizations, people)\n",
    "  matcher = Matcher(nlp.vocab)\n",
    "  org_pattern = [{'POS': 'PROPN'}, {'LEMMA': 'inc'}, {'LEMMA': 'ltd'}, {'LEMMA': 'llc'}]\n",
    "  person_pattern = [{'POS': 'PROPN'}]\n",
    "  matcher.add(\"ORG\", org_pattern)  # Remove unnecessary None argument\n",
    "  matcher.add(\"PERSON\", person_pattern)  # Remove unnecessary None argument\n",
    "\n",
    "  doc.ents = [ent for ent in doc.ents if ent.label_ in (\"ORG\", \"PERSON\")]  # Extract relevant entities\n",
    "  cleaned_text = doc.text_.lower()  # Lowercase text\n",
    "\n",
    "  return cleaned_text, [ent.text for ent in doc.ents]  # Return cleaned text and extracted entities\n",
    "\n",
    "\n",
    "# Apply preprocessing to summaries and extract entities\n",
    "# df[\"summary\"], df[\"entities\"] = zip(*df[\"summary\"].apply(preprocess_text))\n",
    "# Apply preprocessing with a temporary DataFrame\n",
    "# temp_df = df[[\"summary\"]].apply(preprocess_text, axis=1, result_type='expand')\n",
    "temp_df = df[[\"summary\"]].apply(lambda x: preprocess_text(x.iloc[0]), axis=1, result_type='expand')\n",
    "\n",
    "\n",
    "# Assign results back to original DataFrame\n",
    "df[\"summary\"] = temp_df[0]\n",
    "df[\"entities\"] = temp_df[1]\n",
    "\n",
    "# Define number of topics\n",
    "num_topics = 5  # Adjust number of topics as needed\n",
    "\n",
    "# Vectorize Text with TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "text_vectorized = vectorizer.fit_transform(df[\"summary\"])\n",
    "\n",
    "# Train the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(text_vectorized)\n",
    "\n",
    "# Get topics and vocabulary\n",
    "topic_words = []\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "  topic_words.append([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])  # Top 10 words for each topic\n",
    "\n",
    "# Analyze Topics\n",
    "print(f\"Topics identified:\")\n",
    "for i, topic in enumerate(topic_words):\n",
    "  print(f\"Topic {i+1}: {', '.join(topic)}\")\n",
    "\n",
    "# Print first N documents associated with each topic (example)\n",
    "for topic_idx in range(num_topics):\n",
    "  topic_docs = df.loc[df['summary'].apply(lambda x: lda.transform(vectorizer.transform([x]))[:, topic_idx] > 0.2).idxmax(), :]\n",
    "  print(f\"\\nDocuments related to Topic {topic_idx+1}:\")\n",
    "  for i, row in topic_docs.iterrows():\n",
    "    print(f\"\\t- Document ID: {row['id']}, Summary: {row['summary']}\")\n",
    "\n",
    "# Print extracted entities (example)\n",
    "print(\"\\nExtracted Entities:\")\n",
    "entity_counts = {}\n",
    "for entities in df[\"entities\"]:\n",
    "  for entity in entities:\n",
    "    entity_counts[entity] = entity_counts.get(entity, 0) + 1\n",
    "sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\tTop 10 Most Frequent Entities:\")\n",
    "for i, (entity, count) in enumerate(sorted_entities[:10]):\n",
    "  print(f\"\\t\\t- {entity}: {count} mentions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option(\"display.max_rows\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEC Charges Lordstown Motors 0 28 ORG\n",
      "Misleading Investors 34 54 ORG\n",
      "Company’s Flagship Electric Vehicle 61 96 ORG\n",
      "The Securities and Exchange Commission 99 137 ORG\n",
      "today 138 143 DATE\n",
      "Lordstown Motors Corp. 152 174 ORG\n",
      "Lordstown 230 239 GPE\n",
      "Lordstown 289 298 GPE\n",
      "2023 330 334 DATE\n",
      "----------------------------------------------------------------\n",
      "SEC Investor Advisory Committee 0 31 ORG\n",
      "The Securities and Exchange Commission’s 150 190 ORG\n",
      "Investor Advisory Committee 191 218 ORG\n",
      "March 7 249 256 DATE\n",
      "10 a.m. ET 260 270 TIME\n",
      "the SEC Headquarters 274 294 FAC\n",
      "Washington 298 308 GPE\n",
      "D.C. 310 314 GPE\n",
      "SEC 355 358 ORG\n",
      "two 393 396 CARDINAL\n",
      "----------------------------------------------------------------\n",
      "Federal Court Orders Unregistered Pool Operator 0 47 ORG\n",
      "Over $11 Million 73 89 MONEY\n",
      "Forex Fraud 94 105 ORG\n",
      "----------------------------------------------------------------\n",
      "Federal Court Orders 0 20 ORG\n",
      "Salesperson 75 86 PERSON\n",
      "Over $56 Million 94 110 MONEY\n",
      "----------------------------------------------------------------\n",
      "CFTC 0 4 ORG\n",
      "----------------------------------------------------------------\n",
      "Federal Court Orders 0 20 ORG\n",
      "$9 Million 59 69 MONEY\n",
      "Restitution and Penalties 73 98 ORG\n",
      "Forex Fraud 103 114 NORP\n",
      "----------------------------------------------------------------\n",
      "Federal Court Sanctions Nevada Metals Trader for Spoofing in Gold 0 65 ORG\n",
      "----------------------------------------------------------------\n",
      "CFTC 0 4 ORG\n",
      "Australian 12 22 NORP\n",
      "500,000 43 50 MONEY\n",
      "----------------------------------------------------------------\n",
      "CFTC 0 4 ORG\n",
      "KuCoin 13 19 PERSON\n",
      "----------------------------------------------------------------\n",
      "Federal Court Orders 0 20 ORG\n",
      "$3.4 Million 49 61 MONEY\n",
      "Forex, Options Scheme 75 96 ORG\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "content = \"Trinamool Congress leader Mahua Moitra has moved the Supreme Court against her expulsion from the Lok Sabha over the cash-for-query allegations against her. Moitra was ousted from the Parliament last week after the Ethics Committee of the Lok Sabha found her guilty of jeopardising national security by sharing her parliamentary portal's login credentials with businessman Darshan Hiranandani.\"\n",
    "content = \"SEC Charges Lordstown Motors with Misleading Investors about Company’s Flagship Electric Vehicle. The Securities and Exchange Commission today charged Lordstown Motors Corp. with misleading investors about the sales prospects of Lordstown’s flagship electric pickup truck, the Endurance. Lordstown, which filed for bankruptcy in 2023, went public by…\"\n",
    "\n",
    "for ii in range(10):\n",
    "\t# print(ii)\n",
    "\tdoc = nlp(df['summary'][ii])\n",
    "\n",
    "\tfor ent in doc.ents:\n",
    "\t\tprint(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\tprint('----------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling (Over Time):\n",
    "\n",
    "Topic modeling is a technique for identifying latent thematic structures within a collection of documents. Here's how to use it to analyze trends in your article data over time:\n",
    "Preprocess the Text: Clean and prepare your article summaries as mentioned earlier (date formatting, text cleaning).\n",
    "\n",
    "Vectorize the Text: Convert the summaries into a numerical representation that a machine learning model can understand. This can be done using techniques like TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "Run Topic Modeling: Apply a topic modeling algorithm like Latent Dirichlet Allocation (LDA) to the vectorized summaries. This will identify a set of topics and the probability of each word belonging to those topics.\n",
    "\n",
    "Analyze Topics Over Time: Track how the prevalence of these topics changes over the specified time period. This can be done by looking at the distribution of topics within different timeframes (e.g., months, quarters). You can calculate the proportion of articles belonging to each topic for each timeframe and visualize the trends.\n",
    "\n",
    "Tools like LatentDirichletAllocation in scikit-learn (Python) or stm (R) can be used for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
